{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regressões Visual\n",
    "\n",
    "Modelos de regressão podem ser representados visualmente com **diagramas de Venn** (também chamados de diagramas de Ballentine por sua semelhança com o logotipo da cerveja ). Nestes diagramas, cada variável no modelo de regressão é representada por um círculo diferente. Nos modelos básicos, um círculo azul representa a variável dependente (Y), enquanto os círculos vermelhos representam uma variável independente (X). Os tamanhos dos círculos e a maneira como eles se sobrepõem ilustram vários aspectos das variáveis e como eles se relacionam entre si.\n",
    "\n",
    "## Variação\n",
    "\n",
    "### Variação no Y\n",
    "\n",
    "\n",
    "<img src=\"img/var calc w scatter.png\" width=\"450\" />\n",
    "\n",
    "### Separanndo a Variação Total entre explicada e ruido\n",
    "\n",
    "<img src=\"img/partY2.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "O tamanho de cada círculo representa a variação dessa variável. Abaixo, vemos que a variância da variável X2 é maior que a variância de X1.\n",
    "\n",
    "\n",
    "<img src=\"img/vareqs.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "## Covariância / Correlação\n",
    "\n",
    "Quando duas variáveis ​​covary / estão correlacionadas, essa relação é representada pela sobreposição entre os dois círculos. No diagrama básico abaixo, vemos que existe uma relação entre as variáveis ​​Y e X1. A área de sobreposição (A) representa a covariância de X1 e Y. Observe que uma sobreposição em um diagrama de Ballentine mostra apenas que a covariância existe; não indica se a covariância é positiva ou negativa.\n",
    "\n",
    "<img src=\"img/cov-cor eqs.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "O diagrama mais complexo abaixo mostra três variáveis ​​inter-relacionadas.\n",
    "\n",
    "<img src=\"img/cov eqs.png\" width=\"450\" />\n",
    "\n",
    "## Variação explicada (R 2 )\n",
    "Como A representa o grau em que X1 e Y variam juntos, também podemos dizer que A é a porção da variância de Y que é explicada por X1 (ou pela variação em X1). Assim, o R2 da regressão de Y em X1 é igual à área de sobreposição (A) dividida pela área total representando Y (A + B).\n",
    "\n",
    "<img src=\"img/R2 eqs.png\" width=\"450\" />\n",
    "\n",
    "## Valor dos Betas (inclinação da regressão)\n",
    "Lembre-se de que o cálculo da inclinação de regressão depende da variância de X e da covariância entre X e Y. Abaixo, a variância de X é representada pelo círculo vermelho (o total das seções A e C) e a covariância entre X e Y é representada por a sobreposição (seção A). A visualização das partes do Ballentine usadas no cálculo do declive de regressão ajudará mais tarde a entender como as variáveis ​​independentes afetam umas às outras no contexto de regressão múltipla.\n",
    "\n",
    "<img src=\"img/slopecalc.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "## Erro padrão dos Betas\n",
    "\n",
    "Lembre-se de que calcular o erro padrão da inclinação começa com a soma dos termos de erro quadráticos (o SSE).\n",
    "\n",
    "<img src=\"img/SEbformulae.png\" width=\"450\" />\n",
    "\n",
    "Dentro do diagrama de Ballentine, podemos visualizar o SSE como a porção inexplicada da variância de Y.\n",
    "\n",
    "<img src=\"img/SSEvenn.png\" width=\"450\" />\n",
    "\n",
    "Podemos inferir a partir deles que existem três maneiras de reduzir o erro padrão do coeficiente (o que torna estreito o intervalo de confiança ao redor da inclinação): 1. aumentar o tamanho da amostra; 2. explicar mais variância de Y (isto é, adicionar variáveis ​​de controle); ou 3. aumentar a variância de X.\n",
    "\n",
    "## Comparação de modelos\n",
    "O Ballentine pode ilustrar por que os declives de regressão podem mudar quando você adiciona mais variáveis ​​a um modelo. No modelo bivariado que discutimos até agora, a seção A representa a covariância de X1 e Y, que é usada no cálculo da inclinação. Em outras palavras, a seção A é a variação em Y que é explicada por X1.\n",
    "\n",
    "\n",
    "<img src=\"img/cov-cor.png\" width=\"450\" />\n",
    "\n",
    "Compare isso com um modelo que adiciona uma segunda variável independente. No contexto de regressão múltipla, estamos interessados ​​na variação em Y que é explicada de forma única por cada variável independente. Abaixo, a seção A é a variação em Y que é explicada exclusivamente por X1, a seção B é a variação em Y que é explicada exclusivamente por X2, e a seção C é a variação em Y que é explicada em conjunto por X1 e X2. Na regressão múltipla, essa variação em Y que não pode ser explicada apenas por X1 ou X2 (seção C) é retirada dos cálculos de inclinação.\n",
    "\n",
    "<img src=\"img/multireg.png\" width=\"450\" />\n",
    "\n",
    "Como vimos no caso bivariado anterior, ambas as seções A e C seriam atribuídas a X1 se X2 fosse omitido do modelo. Da mesma forma, a seção C (juntamente com a seção B) seria atribuída a X2 se X1 fosse omitido do modelo. Essa é uma covariância contaminada que distorcerá nossas estimativas de inclinação de regressão se X1 ou X2 forem omitidos do modelo.\n",
    "\n",
    "Note também que a adição de uma variável altera ligeiramente a fórmula para calcular os declives de regressão. Abaixo, a fórmula de inclinação à esquerda é para a regressão bivariada e a da direita para a regressão múltipla.\n",
    "\n",
    "\n",
    "<img src=\"img/bivarvsmulti slope calc.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "## Viés da Variável Omitida\n",
    "Como visto na subseção anterior, omitir uma variável de um modelo de regressão pode influenciar as estimativas de inclinação para as variáveis ​​incluídas no modelo. O viés ocorre apenas quando a variável omitida está correlacionada com a variável dependente e com uma das variáveis ​​independentes incluídas.\n",
    "\n",
    "Abaixo, o declive para X1 será polarizado se X2 for omitido do modelo. O viés deriva da covariância contaminada na seção B, mas a direção / tamanho do viés não pode ser determinada a partir do diagrama.\n",
    "\n",
    "<img src=\"img/ovb case 1.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "\n",
    "Abaixo, o declive para X1 não será polarizado se X2 for omitido do modelo.\n",
    "<img src=\"img/ovb case 2.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "\n",
    "Tenha em mente que, em ambos os casos, omitir a variável X2 alterará o erro padrão calculado para X1.\n",
    "## Erro de medição\n",
    "O erro de medição em uma variável pode afetar as estimativas de inclinação e os erros padrão na regressão. Observe que o erro de medição é sempre um erro aleatório. O erro de medição faz com que a variação dessa variável aumente. Como o erro de medição pode surgir em uma variável dependente ou independente, os efeitos de ambas as situações são detalhados nesta seção.\n",
    "\n",
    "O erro de medição na variável dependente causa variação adicional em Y. Nos Ballentines abaixo, isso é ilustrado pelo maior círculo Y à direita. Com base em nossas fórmulas para o declive e erro padrão, podemos inferir do diagrama que as estimativas de inclinação serão idênticas, mas que o erro padrão será maior no caso à direita. O erro padrão inflacionado, por sua vez, tornará menos provável que o coeficiente de inclinação seja estatisticamente significativo.\n",
    "\n",
    "\n",
    "<img src=\"img/dv measerror.png\" width=\"450\" />\n",
    "\n",
    "Erro de medição na variável independente causa variação adicional em X. Nos Ballentines abaixo, isso é refletido pelo círculo X maior à direita. Aqui podemos inferir do diagrama que, para o caso à direita, tanto a estimativa de inclinação quanto o erro padrão diminuirão. Isto é, a inclinação é empurrada para zero (seja positiva ou negativa); isso é chamado de atenuação .\n",
    "<img src=\"img/iv measerror.png\" width=\"450\" />\n",
    "\n",
    "\n",
    "## Multicolinearidade\n",
    "A multicolinearidade surge quando as variáveis ​​independentes em um modelo de regressão estão fortemente correlacionadas entre si. Isso dificulta ver os efeitos isolados de cada uma das variáveis ​​independentes na variável dependente. Isso também causa a inflação dos erros padrão de cada inclinação, o que aumenta os intervalos de confiança correspondentes e torna menos provável que os coeficientes de inclinação sejam estatisticamente significativos. Existe um alto grau de multicolinearidade no modelo abaixo. Quanto maior a multicolinearidade, menor a área B total e, portanto, maior o erro padrão da inclinação.\n",
    "<img src=\"img/multicoll.png\" width=\"450\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
